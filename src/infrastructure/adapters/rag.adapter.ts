import { OpenAIService } from '../services/openai.service.js'
import { PineconeService } from '../services/pinecone.service.js'
import { QueryResult } from '../../domain/entities/query-result.entity.js'
import { VectorSearchFilters } from '../../domain/repositories/vector-database.repository.js'

export interface RAGQueryOptions {
  areas?: string[]
  topK?: number
  filters?: VectorSearchFilters
  systemPrompt?: string
}

export interface RAGResponse {
  question: string
  answer: string
  queryResult: QueryResult
  contextoUsado: string
  totalDocumentosEncontrados: number
}

export class RAGAdapter {
  constructor(
    private readonly openAIService: OpenAIService,
    private readonly pineconeService: PineconeService
  ) {}

  /**
   * Procesa una pregunta usando RAG (Retrieval-Augmented Generation)
   * Combina b√∫squeda vectorial con generaci√≥n de respuestas
   */
  async processQuery(
    question: string,
    options: RAGQueryOptions = {}
  ): Promise<RAGResponse> {
    const {
      areas,
      topK = 10,
      filters,
      systemPrompt
    } = options

    const executionId = `[${new Date().toLocaleTimeString()}.${Date.now() % 1000}]`
    
    try {
      console.log(`RagAdapter.processQuery --> ${executionId} ü§î Procesando pregunta RAG: "${question}"`)
      console.log(`RagAdapter.processQuery --> ${executionId} üîç Buscando ${topK} documentos relevantes...`)

      // 1. Generar embedding para la pregunta
      const queryEmbedding = await this.openAIService.generateEmbedding(question)

      // 2. Buscar documentos similares en Pinecone
      const searchResult = await this.pineconeService.searchSimilarDocuments(
        queryEmbedding,
        areas,
        topK,
        filters
      )

      if (!searchResult.documents || searchResult.documents.length === 0) {
        console.log(`RagAdapter.processQuery --> ${executionId} ‚ùå No se encontraron documentos relevantes`)
        return {
          question,
          answer: 'No se encontraron documentos relevantes para responder tu pregunta. Por favor, intenta reformular tu consulta o verifica que existan documentos relacionados con el tema.',
          queryResult: searchResult,
          contextoUsado: '',
          totalDocumentosEncontrados: 0
        }
      }

      console.log(`RagAdapter.processQuery --> ${executionId} ‚ú® Encontrados ${searchResult.documents.length} documentos relevantes`)
      console.log(`RagAdapter.processQuery --> ${executionId} üìã Fuentes encontradas:`)
      
      searchResult.documents.slice(0, 10).forEach((doc, i) => {
        const pageInfo = doc.page !== 'N/A' ? ` | P√°g. ${doc.page}` : ''
        console.log(`RagAdapter.processQuery --> ${executionId}   ${i + 1}. ${doc.source}${pageInfo} (similitud: ${doc.score.toFixed(3)})`)
        console.log(`RagAdapter.processQuery --> ${executionId}      üìù ${doc.text.substring(0, 150)}...`)
      })

      // 3. Construir contexto con informaci√≥n de p√°ginas
      const context = searchResult.documents
        .map((doc, i) => ` Documento/Source ${doc.source} (P√°gina/Page ${doc.page}): ${doc.text}`)
        .join('\n\n')

      // 4. Generar respuesta usando el LLM con el contexto
      console.log(`RagAdapter.processQuery --> ${executionId} üß† Generando respuesta con IA...`)
      console.log(`RagAdapter.processQuery --> ${executionId} Contexto Usado: ${context}`)
      const answer = await this.openAIService.generateChatCompletion(
        question,
        `${context}`,
        systemPrompt
      )

      console.log(`${executionId} ‚úÖ Respuesta generada exitosamente`)

      return {
        question,
        answer,
        queryResult: searchResult,
        contextoUsado: context,
        totalDocumentosEncontrados: searchResult.documents.length
      }

    } catch (error) {
      console.error(`${executionId} ‚ùå Error en procesamiento RAG:`, error)
      throw new Error(`Error en procesamiento RAG: ${error}`)
    }
  }

  /**
   * Verifica que todos los servicios est√©n funcionando
   */
  async healthCheck(): Promise<{
    openai: boolean
    pinecone: boolean
    overall: boolean
  }> {
    try {
      console.log('üîç Verificando salud de servicios RAG...')
      
      const [openaiStatus, pineconeStatus] = await Promise.all([
        this.openAIService.ping(),
        this.pineconeService.ping()
      ])

      const overall = openaiStatus && pineconeStatus

      console.log(`‚úÖ Estado de servicios - OpenAI: ${openaiStatus ? '‚úÖ' : '‚ùå'}, Pinecone: ${pineconeStatus ? '‚úÖ' : '‚ùå'}`)

      return {
        openai: openaiStatus,
        pinecone: pineconeStatus,
        overall
      }
    } catch (error) {
      console.error('‚ùå Error en health check:', error)
      return {
        openai: false,
        pinecone: false,
        overall: false
      }
    }
  }
}
